# -*- coding: utf-8 -*-
"""Customers.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18YoBjUHRveDnfQrLl7cPyd3nmWwweqH9
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# this is all the libraries that we need
from google.colab import drive
drive.mount('/content/drive')


sns.set(style="whitegrid")

file_path = "/content/drive/MyDrive/dataset.csv"

df = pd.read_csv(file_path, index_col="customerID")

# Reading the dataset and adjusting the index column


df.TotalCharges.value_counts()
df["TotalCharges"].dtype
df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')
#changing the datatype of the TotalCharges from object to float

df['TotalCharges'] = df["TotalCharges"].fillna(df["TotalCharges"].median())
#datacleaning
df.info()
df.isnull().sum()
# visualizing the relations
sns.boxplot(x="Churn", y="tenure",data=df)
plt.title("Churn Distribution")
plt.show()
df.groupby("Churn")["tenure"].median()
sns.countplot(x="Contract", hue="Churn", data=df)
plt.show()

# The scatter function graph
sns.scatterplot(
    x = "tenure",
    y = "MonthlyCharges",
    hue = "Churn",
    data = df,
    alpha = 0.5
)
df_encoded = pd.get_dummies(df,drop_first = True)
# one hot encoding
from sklearn.model_selection import train_test_split
x = df_encoded.drop("Churn_Yes",axis = 1)
y = df_encoded["Churn_Yes"]
df_encoded.columns
x_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2, random_state = 42)
# Splitting the data for testing and training
from sklearn.linear_model import LogisticRegression
model = LogisticRegression(max_iter=3000)
# Using the logisticRegression model as classical model
model.fit(x_train,y_train)
# Training the model according the dataset
y_pred = model.predict(x_test)
from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))
# Printing the result

"""Level 1 and 2 is done.
I have used the LogisticRegression model.
visualization of the relations is done.
The accuracy average is around 81%.
F1 score is also mentioned there in the result




"""

